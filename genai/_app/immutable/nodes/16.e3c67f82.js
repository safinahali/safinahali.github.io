import{S as Vi,i as Wi,s as Xi,k as i,q as r,a as d,l as o,m as s,r as l,h as t,c as h,n,b as u,G as e,M as Ja}from"../chunks/index.9dc8c717.js";function Qi(Fi){let v,y,ve,lt,dt,Ne,ee,ht,Ue,x,b,we,ct,ut,Be,te,ft,Oe,L,gt,M,mt,pt,Fe,S,E,ye,vt,wt,Ve,m,R,be,yt,bt,p,Ee,C,Et,kt,ke,_t,It,_e,At,xt,Ie,Lt,St,G,Ae,Tt,Pt,k,xe,q,Dt,Ht,ae,Mt,j,Rt,Ct,Le,Gt,qt,z,Se,jt,zt,K,Te,Kt,Nt,Pe,Ut,Bt,N,De,Ot,Ft,U,He,B,Vt,Wt,Me,Xt,Qt,O,Re,Jt,Yt,Ce,Ge,Zt,We,T,_,qe,$t,ea,Xe,ie,ta,Qe,P,I,je,aa,ia,Je,f,oe,oa,F,sa,na,se,ra,V,la,da,ne,ha,W,ca,ua,re,fa,X,ga,ma,le,pa,Q,va,wa,de,ya,J,ba,Ea,he,ka,Y,_a,Ia,ce,Aa,Z,xa,La,ue,Sa,$,Ta,Ye,fe,Pa,Ze,D,A,ze,Da,Ha,$e,ge,me,Ke,Ma;return{c(){v=i("h2"),y=i("a"),ve=i("span"),lt=r("#"),dt=r("Theme and goals"),Ne=d(),ee=i("p"),ht=r("Text-to-image generation technologies such as Stable Diffusion, DALL-E and Midjourney have become extremely popular in recent months garnering interest from people even outside of the AI community, including educators and k12 students. These powerful tools are able to generate high quality visuals from natural language prompts and are open to access for anyone. These tools can have infinite creative potential when used by k12 learners and educators but are also accompanied by serious ethical implications. However, currently educators and their students don’t necessarily have a good understanding of how these tools work or how they can be possibly used or misused. In this tutorial, we will demystify text-to-image generative tools for k12 educators as well as learning science researchers, and work along with educators to design teaching lessons and curricula around bringing these tools to the classroom. The goal of the workshop is for educators and k12 learning researchers to gain a clear understanding of how these generative tools work, and co-designing with them learning tools, lessons or curricula to teach k12 students about them."),Ue=d(),x=i("h2"),b=i("a"),we=i("span"),ct=r("#"),ut=r("Background"),Be=d(),te=i("p"),ft=r("Text-to-image generation platforms are breaking new ground in AI tools that empower beginners by letting anyone easily create images with professional quality appearance. These platforms use massive datasets comprising labeled images scraped from art blogs, museum websites, fanfiction sites, etc. The datasets are used to train chains of neural networks and large language models, which learn to generate novel images that can mimic human gestures, tones, faces, and voices with uncanny accuracy. AI Literacy - the ability to understand and work with these generative AI tools - is of multi-disciplinary interest and has the potential to open doors to a large number of future careers."),Oe=d(),L=i("p"),gt=r("From an educational perspective, text-to-image generation offers students a double edged sword: powerful new tools for self-expression, infused with the societal biases baked into the training data. How to help students (and their teachers) demystify these tools and develop their AI Literacy along with an understanding of the ethical and socio-technical implications of bias in AI? This is a pivotal question for educators and researchers in the learning sciences as generative AI platforms, such as DALL-E and ChatGPT, have recently proven themselves able to disrupt traditional processes with which we, in the learning sciences, measure learning outcomes. How will students express their thinking or refine their craftsmanship if an AI can generate their work for them? This tutorial is designed to engage educators and learning scientists in possible answers to these questions by sharing insights gleaned from a. recent research on AI Literacy, specifically generative AI, in middle and high school settings, b. current research on professional development models for building teachers’ AI Literacy, and c. discussion and projects created during a pilot seminar on text-to-image generation run by researchers at MIT’s Responsible AI for Social Empowerment and Education (RAISE) initiative "),M=i("a"),mt=r("https://image-gen.github.io/"),pt=r(". The tutorial will culminate in a project-based activity drawing from constructionism and computational action to expose participants to culturally-sustaining, hands-on methods of teaching about text-to-image generative platforms, and empower learners to critically and creatively engage text-to-image platforms as tools for communication and critical engagement with media."),Fe=d(),S=i("h2"),E=i("a"),ye=i("span"),vt=r("#"),wt=r("Outline of planned activities"),Ve=d(),m=i("ol"),R=i("li"),be=i("p"),yt=r("Introduction: Exploring generative AI models and their relevance in K-12 AI Literacy"),bt=d(),p=i("ul"),Ee=i("li"),C=i("a"),Et=r("Session 1 Slides"),kt=d(),ke=i("li"),_t=r("What are language inference and visual generative AI models and how do they work."),It=d(),_e=i("li"),At=r("Examples of generative media."),xt=d(),Ie=i("li"),Lt=r("Motivation for teaching this to K-12 students: New media and possibilities for creation and computational creativity, as well as their ethical considerations."),St=d(),G=i("li"),Ae=i("p"),Tt=r("Experimenting with the latest generative AI platforms"),Pt=d(),k=i("ul"),xe=i("li"),q=i("a"),Dt=r("Session 2 Slides"),Ht=d(),ae=i("li"),Mt=r("Survey and experiment with some existing platforms: DALL-E 2, Stable Diffusion, Midjourney, & NightCafe and complete two activities using them: Creative AI Storytelling and Self-portraits: Description and examples of these two activities taken from our pilot seminar are linked "),j=i("a"),Rt=r("here."),Ct=d(),Le=i("li"),Gt=r("Reflect on your experience of creating images for the activities above - what role did the AI tool play, how this technology can be used or misused, and how is it relevant to k12 students."),qt=d(),z=i("li"),Se=i("p"),jt=r("Understanding how the technology behind text-to-image generation tools works"),zt=d(),K=i("ul"),Te=i("li"),Kt=r("How do generative algorithms like stable diffusion work?"),Nt=d(),Pe=i("li"),Ut=r("How do natural language models fit into this?"),Bt=d(),N=i("li"),De=i("p"),Ot=r("K-12 generative AI literacy"),Ft=d(),U=i("ul"),He=i("li"),B=i("a"),Vt=r("Session 4 Slides"),Wt=d(),Me=i("li"),Xt=r("Case studies: Exploring existing Creative AI curriculum. Discussing learning goals and pedagogical methods used in previous work."),Qt=d(),O=i("li"),Re=i("p"),Jt=r("Design and share proposals for creative AI literacy including target age group, learning goals, tools used, learning activities & assessment."),Yt=d(),Ce=i("ul"),Ge=i("li"),Zt=r("Deliverables will include curricula, learning lessons, interactive tools or assessment methods."),We=d(),T=i("h2"),_=i("a"),qe=i("span"),$t=r("#"),ea=r("Expxected outcomes and contributions"),Xe=d(),ie=i("p"),ta=r("Educators and researchers will gain an understanding of how text-to-image generation tools work and what their societal and ethical implications are. They will brainstorm techniques to bring these tools to the classrooms and teach students about the creative potential and responsible use of generative AI tools."),Qe=d(),P=i("h2"),I=i("a"),je=i("span"),aa=r("#"),ia=r("Resources and links"),Je=d(),f=i("ol"),oe=i("li"),oa=r("(Recommended Read!!) Excellent article from MIT Technology Review summarizing the advances in this field: "),F=i("a"),sa=r("Generative AI is changing everything. But what’s left when the hype is gone?"),na=d(),se=i("li"),ra=r("How do Diffusion models work: "),V=i("a"),la=r("Link."),da=d(),ne=i("li"),ha=r("Greg Rutkowski: This artist is dominating AI-generated art. And he’s not happy about it. Visit "),W=i("a"),ca=r("here."),ua=d(),re=i("li"),fa=r("Diffusion models explained: "),X=i("a"),ga=r("Video."),ma=d(),le=i("li"),pa=r("Illustrated Stable Diffusion (a bit more technical): "),Q=i("a"),va=r("Link."),wa=d(),de=i("li"),ya=r("Diffusion Models: A Practical Guide: "),J=i("a"),ba=r("Link."),Ea=d(),he=i("li"),ka=r("CLIP (connecting text and images): "),Y=i("a"),_a=r("Link."),Ia=d(),ce=i("li"),Aa=r("Paper: High Resolution Image Synthesis with Latent Diffusion Models (more technical but a great resource): "),Z=i("a"),xa=r("Link."),La=d(),ue=i("li"),Sa=r("If you want to play with idioms extension that Parker demo’d, follow the Adding Extensions instructions "),$=i("a"),Ta=r("here."),Ye=d(),fe=i("p"),Pa=r("More to come soon!"),Ze=d(),D=i("h2"),A=i("a"),ze=i("span"),Da=r("#"),Ha=r("References"),$e=d(),ge=i("ol"),me=i("li"),Ke=i("a"),Ma=r("ISLS 2023 Tutorial: Demystifying Text-to-Image generation for K12 educators."),this.h()},l(a){v=o(a,"H2",{id:!0});var c=s(v);y=o(c,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var Ya=s(y);ve=o(Ya,"SPAN",{});var Za=s(ve);lt=l(Za,"#"),Za.forEach(t),Ya.forEach(t),dt=l(c,"Theme and goals"),c.forEach(t),Ne=h(a),ee=o(a,"P",{});var $a=s(ee);ht=l($a,"Text-to-image generation technologies such as Stable Diffusion, DALL-E and Midjourney have become extremely popular in recent months garnering interest from people even outside of the AI community, including educators and k12 students. These powerful tools are able to generate high quality visuals from natural language prompts and are open to access for anyone. These tools can have infinite creative potential when used by k12 learners and educators but are also accompanied by serious ethical implications. However, currently educators and their students don’t necessarily have a good understanding of how these tools work or how they can be possibly used or misused. In this tutorial, we will demystify text-to-image generative tools for k12 educators as well as learning science researchers, and work along with educators to design teaching lessons and curricula around bringing these tools to the classroom. The goal of the workshop is for educators and k12 learning researchers to gain a clear understanding of how these generative tools work, and co-designing with them learning tools, lessons or curricula to teach k12 students about them."),$a.forEach(t),Ue=h(a),x=o(a,"H2",{id:!0});var Ra=s(x);b=o(Ra,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var ei=s(b);we=o(ei,"SPAN",{});var ti=s(we);ct=l(ti,"#"),ti.forEach(t),ei.forEach(t),ut=l(Ra,"Background"),Ra.forEach(t),Be=h(a),te=o(a,"P",{});var ai=s(te);ft=l(ai,"Text-to-image generation platforms are breaking new ground in AI tools that empower beginners by letting anyone easily create images with professional quality appearance. These platforms use massive datasets comprising labeled images scraped from art blogs, museum websites, fanfiction sites, etc. The datasets are used to train chains of neural networks and large language models, which learn to generate novel images that can mimic human gestures, tones, faces, and voices with uncanny accuracy. AI Literacy - the ability to understand and work with these generative AI tools - is of multi-disciplinary interest and has the potential to open doors to a large number of future careers."),ai.forEach(t),Oe=h(a),L=o(a,"P",{});var et=s(L);gt=l(et,"From an educational perspective, text-to-image generation offers students a double edged sword: powerful new tools for self-expression, infused with the societal biases baked into the training data. How to help students (and their teachers) demystify these tools and develop their AI Literacy along with an understanding of the ethical and socio-technical implications of bias in AI? This is a pivotal question for educators and researchers in the learning sciences as generative AI platforms, such as DALL-E and ChatGPT, have recently proven themselves able to disrupt traditional processes with which we, in the learning sciences, measure learning outcomes. How will students express their thinking or refine their craftsmanship if an AI can generate their work for them? This tutorial is designed to engage educators and learning scientists in possible answers to these questions by sharing insights gleaned from a. recent research on AI Literacy, specifically generative AI, in middle and high school settings, b. current research on professional development models for building teachers’ AI Literacy, and c. discussion and projects created during a pilot seminar on text-to-image generation run by researchers at MIT’s Responsible AI for Social Empowerment and Education (RAISE) initiative "),M=o(et,"A",{href:!0,rel:!0});var ii=s(M);mt=l(ii,"https://image-gen.github.io/"),ii.forEach(t),pt=l(et,". The tutorial will culminate in a project-based activity drawing from constructionism and computational action to expose participants to culturally-sustaining, hands-on methods of teaching about text-to-image generative platforms, and empower learners to critically and creatively engage text-to-image platforms as tools for communication and critical engagement with media."),et.forEach(t),Fe=h(a),S=o(a,"H2",{id:!0});var Ca=s(S);E=o(Ca,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var oi=s(E);ye=o(oi,"SPAN",{});var si=s(ye);vt=l(si,"#"),si.forEach(t),oi.forEach(t),wt=l(Ca,"Outline of planned activities"),Ca.forEach(t),Ve=h(a),m=o(a,"OL",{});var w=s(m);R=o(w,"LI",{});var tt=s(R);be=o(tt,"P",{});var ni=s(be);yt=l(ni,"Introduction: Exploring generative AI models and their relevance in K-12 AI Literacy"),ni.forEach(t),bt=h(tt),p=o(tt,"UL",{});var H=s(p);Ee=o(H,"LI",{});var ri=s(Ee);C=o(ri,"A",{href:!0,target:!0});var li=s(C);Et=l(li,"Session 1 Slides"),li.forEach(t),ri.forEach(t),kt=h(H),ke=o(H,"LI",{});var di=s(ke);_t=l(di,"What are language inference and visual generative AI models and how do they work."),di.forEach(t),It=h(H),_e=o(H,"LI",{});var hi=s(_e);At=l(hi,"Examples of generative media."),hi.forEach(t),xt=h(H),Ie=o(H,"LI",{});var ci=s(Ie);Lt=l(ci,"Motivation for teaching this to K-12 students: New media and possibilities for creation and computational creativity, as well as their ethical considerations."),ci.forEach(t),H.forEach(t),tt.forEach(t),St=h(w),G=o(w,"LI",{});var at=s(G);Ae=o(at,"P",{});var ui=s(Ae);Tt=l(ui,"Experimenting with the latest generative AI platforms"),ui.forEach(t),Pt=h(at),k=o(at,"UL",{});var pe=s(k);xe=o(pe,"LI",{});var fi=s(xe);q=o(fi,"A",{href:!0,target:!0});var gi=s(q);Dt=l(gi,"Session 2 Slides"),gi.forEach(t),fi.forEach(t),Ht=h(pe),ae=o(pe,"LI",{});var Ga=s(ae);Mt=l(Ga,"Survey and experiment with some existing platforms: DALL-E 2, Stable Diffusion, Midjourney, & NightCafe and complete two activities using them: Creative AI Storytelling and Self-portraits: Description and examples of these two activities taken from our pilot seminar are linked "),j=o(Ga,"A",{href:!0,target:!0});var mi=s(j);Rt=l(mi,"here."),mi.forEach(t),Ga.forEach(t),Ct=h(pe),Le=o(pe,"LI",{});var pi=s(Le);Gt=l(pi,"Reflect on your experience of creating images for the activities above - what role did the AI tool play, how this technology can be used or misused, and how is it relevant to k12 students."),pi.forEach(t),pe.forEach(t),at.forEach(t),qt=h(w),z=o(w,"LI",{});var it=s(z);Se=o(it,"P",{});var vi=s(Se);jt=l(vi,"Understanding how the technology behind text-to-image generation tools works"),vi.forEach(t),zt=h(it),K=o(it,"UL",{});var ot=s(K);Te=o(ot,"LI",{});var wi=s(Te);Kt=l(wi,"How do generative algorithms like stable diffusion work?"),wi.forEach(t),Nt=h(ot),Pe=o(ot,"LI",{});var yi=s(Pe);Ut=l(yi,"How do natural language models fit into this?"),yi.forEach(t),ot.forEach(t),it.forEach(t),Bt=h(w),N=o(w,"LI",{});var st=s(N);De=o(st,"P",{});var bi=s(De);Ot=l(bi,"K-12 generative AI literacy"),bi.forEach(t),Ft=h(st),U=o(st,"UL",{});var nt=s(U);He=o(nt,"LI",{});var Ei=s(He);B=o(Ei,"A",{href:!0,target:!0});var ki=s(B);Vt=l(ki,"Session 4 Slides"),ki.forEach(t),Ei.forEach(t),Wt=h(nt),Me=o(nt,"LI",{});var _i=s(Me);Xt=l(_i,"Case studies: Exploring existing Creative AI curriculum. Discussing learning goals and pedagogical methods used in previous work."),_i.forEach(t),nt.forEach(t),st.forEach(t),Qt=h(w),O=o(w,"LI",{});var rt=s(O);Re=o(rt,"P",{});var Ii=s(Re);Jt=l(Ii,"Design and share proposals for creative AI literacy including target age group, learning goals, tools used, learning activities & assessment."),Ii.forEach(t),Yt=h(rt),Ce=o(rt,"UL",{});var Ai=s(Ce);Ge=o(Ai,"LI",{});var xi=s(Ge);Zt=l(xi,"Deliverables will include curricula, learning lessons, interactive tools or assessment methods."),xi.forEach(t),Ai.forEach(t),rt.forEach(t),w.forEach(t),We=h(a),T=o(a,"H2",{id:!0});var qa=s(T);_=o(qa,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var Li=s(_);qe=o(Li,"SPAN",{});var Si=s(qe);$t=l(Si,"#"),Si.forEach(t),Li.forEach(t),ea=l(qa,"Expxected outcomes and contributions"),qa.forEach(t),Xe=h(a),ie=o(a,"P",{});var Ti=s(ie);ta=l(Ti,"Educators and researchers will gain an understanding of how text-to-image generation tools work and what their societal and ethical implications are. They will brainstorm techniques to bring these tools to the classrooms and teach students about the creative potential and responsible use of generative AI tools."),Ti.forEach(t),Qe=h(a),P=o(a,"H2",{id:!0});var ja=s(P);I=o(ja,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var Pi=s(I);je=o(Pi,"SPAN",{});var Di=s(je);aa=l(Di,"#"),Di.forEach(t),Pi.forEach(t),ia=l(ja,"Resources and links"),ja.forEach(t),Je=h(a),f=o(a,"OL",{});var g=s(f);oe=o(g,"LI",{});var za=s(oe);oa=l(za,"(Recommended Read!!) Excellent article from MIT Technology Review summarizing the advances in this field: "),F=o(za,"A",{href:!0,target:!0});var Hi=s(F);sa=l(Hi,"Generative AI is changing everything. But what’s left when the hype is gone?"),Hi.forEach(t),za.forEach(t),na=h(g),se=o(g,"LI",{});var Ka=s(se);ra=l(Ka,"How do Diffusion models work: "),V=o(Ka,"A",{href:!0,target:!0});var Mi=s(V);la=l(Mi,"Link."),Mi.forEach(t),Ka.forEach(t),da=h(g),ne=o(g,"LI",{});var Na=s(ne);ha=l(Na,"Greg Rutkowski: This artist is dominating AI-generated art. And he’s not happy about it. Visit "),W=o(Na,"A",{href:!0,target:!0});var Ri=s(W);ca=l(Ri,"here."),Ri.forEach(t),Na.forEach(t),ua=h(g),re=o(g,"LI",{});var Ua=s(re);fa=l(Ua,"Diffusion models explained: "),X=o(Ua,"A",{href:!0,target:!0});var Ci=s(X);ga=l(Ci,"Video."),Ci.forEach(t),Ua.forEach(t),ma=h(g),le=o(g,"LI",{});var Ba=s(le);pa=l(Ba,"Illustrated Stable Diffusion (a bit more technical): "),Q=o(Ba,"A",{href:!0,target:!0});var Gi=s(Q);va=l(Gi,"Link."),Gi.forEach(t),Ba.forEach(t),wa=h(g),de=o(g,"LI",{});var Oa=s(de);ya=l(Oa,"Diffusion Models: A Practical Guide: "),J=o(Oa,"A",{href:!0,target:!0});var qi=s(J);ba=l(qi,"Link."),qi.forEach(t),Oa.forEach(t),Ea=h(g),he=o(g,"LI",{});var Fa=s(he);ka=l(Fa,"CLIP (connecting text and images): "),Y=o(Fa,"A",{href:!0,target:!0});var ji=s(Y);_a=l(ji,"Link."),ji.forEach(t),Fa.forEach(t),Ia=h(g),ce=o(g,"LI",{});var Va=s(ce);Aa=l(Va,"Paper: High Resolution Image Synthesis with Latent Diffusion Models (more technical but a great resource): "),Z=o(Va,"A",{href:!0,target:!0});var zi=s(Z);xa=l(zi,"Link."),zi.forEach(t),Va.forEach(t),La=h(g),ue=o(g,"LI",{});var Wa=s(ue);Sa=l(Wa,"If you want to play with idioms extension that Parker demo’d, follow the Adding Extensions instructions "),$=o(Wa,"A",{href:!0,target:!0});var Ki=s($);Ta=l(Ki,"here."),Ki.forEach(t),Wa.forEach(t),g.forEach(t),Ye=h(a),fe=o(a,"P",{});var Ni=s(fe);Pa=l(Ni,"More to come soon!"),Ni.forEach(t),Ze=h(a),D=o(a,"H2",{id:!0});var Xa=s(D);A=o(Xa,"A",{class:!0,title:!0,"aria-hidden":!0,href:!0});var Ui=s(A);ze=o(Ui,"SPAN",{});var Bi=s(ze);Da=l(Bi,"#"),Bi.forEach(t),Ui.forEach(t),Ha=l(Xa,"References"),Xa.forEach(t),$e=h(a),ge=o(a,"OL",{});var Oi=s(ge);me=o(Oi,"LI",{});var Qa=s(me);Ke=o(Qa,"A",{href:!0}),s(Ke).forEach(t),Ma=l(Qa,"ISLS 2023 Tutorial: Demystifying Text-to-Image generation for K12 educators."),Qa.forEach(t),Oi.forEach(t),this.h()},h(){n(y,"class","heading-link"),n(y,"title","Permalink"),n(y,"aria-hidden","true"),n(y,"href","#theme-and-goals"),n(v,"id","theme-and-goals"),n(b,"class","heading-link"),n(b,"title","Permalink"),n(b,"aria-hidden","true"),n(b,"href","#background"),n(x,"id","background"),n(M,"href","https://image-gen.github.io/"),n(M,"rel","nofollow"),n(E,"class","heading-link"),n(E,"title","Permalink"),n(E,"aria-hidden","true"),n(E,"href","#outline-of-planned-activities"),n(S,"id","outline-of-planned-activities"),n(C,"href","https://docs.google.com/presentation/d/1V-G28MT0Po0UDKfzBxAKpZgAwQv85TS4Sw6BgXkz56E/copy"),n(C,"target","_blank"),n(q,"href","https://docs.google.com/presentation/d/1LJM4nnX2e6QuGYHanO7MoCrlPBoueeGxvRSyAFnL3yU/copy"),n(q,"target","_blank"),n(j,"href","https://docs.google.com/document/d/1USWtimz9z5FXqla08zm7b9khBIBl24hfWTX1HHfJMFE/copy"),n(j,"target","_blank"),n(B,"href","https://docs.google.com/presentation/d/1GzPQSdAPzkPnoTJ6znGxXbZiQx5lWFOk-TkYdC0pj-M/copy"),n(B,"target","_blank"),n(_,"class","heading-link"),n(_,"title","Permalink"),n(_,"aria-hidden","true"),n(_,"href","#expxected-outcomes-and-contributions"),n(T,"id","expxected-outcomes-and-contributions"),n(I,"class","heading-link"),n(I,"title","Permalink"),n(I,"aria-hidden","true"),n(I,"href","#resources-and-links"),n(P,"id","resources-and-links"),n(F,"href","https://www.technologyreview.com/2022/12/16/1065005/generative-ai-revolution-art/"),n(F,"target","_blank"),n(V,"href","https://towardsdatascience.com/diffusion-models-made-easy-8414298ce4da"),n(V,"target","_blank"),n(W,"href","https://www.technologyreview.com/2022/09/16/1059598/this-artist-is-dominating-ai-generated-art-and-hes-not-happy-about-it/"),n(W,"target","_blank"),n(X,"href","https://www.youtube.com/watch?v=yTAMrHVG1ew"),n(X,"target","_blank"),n(Q,"href","https://jalammar.github.io/illustrated-stable-diffusion/"),n(Q,"target","_blank"),n(J,"href","https://scale.com/guides/diffusion-models-guide"),n(J,"target","_blank"),n(Y,"href","https://openai.com/blog/clip/"),n(Y,"target","_blank"),n(Z,"href","https://arxiv.org/pdf/2112.10752.pdf"),n(Z,"target","_blank"),n($,"href","https://en.scratch-wiki.info/wiki/Extension_and_look_for_DallE-dioms"),n($,"target","_blank"),n(A,"class","heading-link"),n(A,"title","Permalink"),n(A,"aria-hidden","true"),n(A,"href","#references"),n(D,"id","references"),n(Ke,"href","https://mitmedialab.github.io/genai-isls/")},m(a,c){u(a,v,c),e(v,y),e(y,ve),e(ve,lt),e(v,dt),u(a,Ne,c),u(a,ee,c),e(ee,ht),u(a,Ue,c),u(a,x,c),e(x,b),e(b,we),e(we,ct),e(x,ut),u(a,Be,c),u(a,te,c),e(te,ft),u(a,Oe,c),u(a,L,c),e(L,gt),e(L,M),e(M,mt),e(L,pt),u(a,Fe,c),u(a,S,c),e(S,E),e(E,ye),e(ye,vt),e(S,wt),u(a,Ve,c),u(a,m,c),e(m,R),e(R,be),e(be,yt),e(R,bt),e(R,p),e(p,Ee),e(Ee,C),e(C,Et),e(p,kt),e(p,ke),e(ke,_t),e(p,It),e(p,_e),e(_e,At),e(p,xt),e(p,Ie),e(Ie,Lt),e(m,St),e(m,G),e(G,Ae),e(Ae,Tt),e(G,Pt),e(G,k),e(k,xe),e(xe,q),e(q,Dt),e(k,Ht),e(k,ae),e(ae,Mt),e(ae,j),e(j,Rt),e(k,Ct),e(k,Le),e(Le,Gt),e(m,qt),e(m,z),e(z,Se),e(Se,jt),e(z,zt),e(z,K),e(K,Te),e(Te,Kt),e(K,Nt),e(K,Pe),e(Pe,Ut),e(m,Bt),e(m,N),e(N,De),e(De,Ot),e(N,Ft),e(N,U),e(U,He),e(He,B),e(B,Vt),e(U,Wt),e(U,Me),e(Me,Xt),e(m,Qt),e(m,O),e(O,Re),e(Re,Jt),e(O,Yt),e(O,Ce),e(Ce,Ge),e(Ge,Zt),u(a,We,c),u(a,T,c),e(T,_),e(_,qe),e(qe,$t),e(T,ea),u(a,Xe,c),u(a,ie,c),e(ie,ta),u(a,Qe,c),u(a,P,c),e(P,I),e(I,je),e(je,aa),e(P,ia),u(a,Je,c),u(a,f,c),e(f,oe),e(oe,oa),e(oe,F),e(F,sa),e(f,na),e(f,se),e(se,ra),e(se,V),e(V,la),e(f,da),e(f,ne),e(ne,ha),e(ne,W),e(W,ca),e(f,ua),e(f,re),e(re,fa),e(re,X),e(X,ga),e(f,ma),e(f,le),e(le,pa),e(le,Q),e(Q,va),e(f,wa),e(f,de),e(de,ya),e(de,J),e(J,ba),e(f,Ea),e(f,he),e(he,ka),e(he,Y),e(Y,_a),e(f,Ia),e(f,ce),e(ce,Aa),e(ce,Z),e(Z,xa),e(f,La),e(f,ue),e(ue,Sa),e(ue,$),e($,Ta),u(a,Ye,c),u(a,fe,c),e(fe,Pa),u(a,Ze,c),u(a,D,c),e(D,A),e(A,ze),e(ze,Da),e(D,Ha),u(a,$e,c),u(a,ge,c),e(ge,me),e(me,Ke),e(me,Ma)},p:Ja,i:Ja,o:Ja,d(a){a&&t(v),a&&t(Ne),a&&t(ee),a&&t(Ue),a&&t(x),a&&t(Be),a&&t(te),a&&t(Oe),a&&t(L),a&&t(Fe),a&&t(S),a&&t(Ve),a&&t(m),a&&t(We),a&&t(T),a&&t(Xe),a&&t(ie),a&&t(Qe),a&&t(P),a&&t(Je),a&&t(f),a&&t(Ye),a&&t(fe),a&&t(Ze),a&&t(D),a&&t($e),a&&t(ge)}}}class Yi extends Vi{constructor(v){super(),Wi(this,v,null,Qi,Xi,{})}}export{Yi as component};
